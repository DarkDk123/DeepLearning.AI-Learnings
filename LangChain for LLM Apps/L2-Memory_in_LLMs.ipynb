{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a786c77c",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "Language Models are inherently stateless, and they do not remember previous conversations, because they generate next tokens based on the given existing tokens!\n",
    "\n",
    "That's why we have memory, to keep track of the conversation and feed it to the LLM every time we make a new prompt to it!\n",
    "\n",
    "## Outline\n",
    "* ConversationBufferMemory\n",
    "* ConversationBufferWindowMemory\n",
    "* ConversationTokenBufferMemory\n",
    "* ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297dcd5",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f518f5",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing for ENV variables\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301270b-7a69-40f8-9934-f840499b6ae9",
   "metadata": {},
   "source": [
    "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0924a2-ce2d-44ba-a806-c1195720c237",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\" # This in my case\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ad6fe2",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As this is an old course, Langchain synxtax is little different!\n",
    "# Importing langchain components.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88bdf13d",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating chat object, Buffer Memory and\n",
    "# Converstation chain using them!\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True # Let's know how messages are stored!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db24677d",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, i'm Dipesh and i'm very interested in Tech, Space & philosophy!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Dipesh! It's great to meet someone with such diverse interests. Tech, space, and philosophy are all fascinating topics. What specifically about these subjects interests you the most?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting the conversation!\n",
    "conversation.predict(input=\"Hi, i'm Dipesh and i'm very interested in Tech, Space & philosophy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc3ef937",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, i'm Dipesh and i'm very interested in Tech, Space & philosophy!\n",
      "AI: Hello Dipesh! It's great to meet someone with such diverse interests. Tech, space, and philosophy are all fascinating topics. What specifically about these subjects interests you the most?\n",
      "Human: There uncertainity, yet power to create, awake & destroy\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's a very profound observation, Dipesh. The balance between uncertainty and the power to create, awaken, and destroy is indeed a complex and intriguing aspect of life. In philosophy, this concept can be explored through existentialism or even through the study of ethics and morality. In tech, this balance can be seen in the development of artificial intelligence or in the ethical considerations of new technologies. And in space, the unknown and the potential for discovery and exploration highlight this delicate balance as well. It's truly fascinating how these themes intersect across different disciplines.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"There uncertainity, yet power to create, awake & destroy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf3339a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, i'm Dipesh and i'm very interested in Tech, Space & philosophy!\n",
      "AI: Hello Dipesh! It's great to meet someone with such diverse interests. Tech, space, and philosophy are all fascinating topics. What specifically about these subjects interests you the most?\n",
      "Human: There uncertainity, yet power to create, awake & destroy\n",
      "AI: That's a very profound observation, Dipesh. The balance between uncertainty and the power to create, awaken, and destroy is indeed a complex and intriguing aspect of life. In philosophy, this concept can be explored through existentialism or even through the study of ethics and morality. In tech, this balance can be seen in the development of artificial intelligence or in the ethical considerations of new technologies. And in space, the unknown and the potential for discovery and exploration highlight this delicate balance as well. It's truly fascinating how these themes intersect across different disciplines.\n",
      "Human: Do you remember? What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, your name is Dipesh.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Of course it'll remember me now!!\n",
    "conversation.predict(input=\"Do you remember? What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2529400d",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, i'm Dipesh and i'm very interested in Tech, Space & philosophy!\n",
      "AI: Hello Dipesh! It's great to meet someone with such diverse interests. Tech, space, and philosophy are all fascinating topics. What specifically about these subjects interests you the most?\n",
      "Human: There uncertainity, yet power to create, awake & destroy\n",
      "AI: That's a very profound observation, Dipesh. The balance between uncertainty and the power to create, awaken, and destroy is indeed a complex and intriguing aspect of life. In philosophy, this concept can be explored through existentialism or even through the study of ethics and morality. In tech, this balance can be seen in the development of artificial intelligence or in the ethical considerations of new technologies. And in space, the unknown and the potential for discovery and exploration highlight this delicate balance as well. It's truly fascinating how these themes intersect across different disciplines.\n",
      "Human: Do you remember? What is my name?\n",
      "AI: Yes, your name is Dipesh.\n"
     ]
    }
   ],
   "source": [
    "# Show the conversation!\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5018cb0a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, i'm Dipesh and i'm very interested in Tech, Space & philosophy!\\nAI: Hello Dipesh! It's great to meet someone with such diverse interests. Tech, space, and philosophy are all fascinating topics. What specifically about these subjects interests you the most?\\nHuman: There uncertainity, yet power to create, awake & destroy\\nAI: That's a very profound observation, Dipesh. The balance between uncertainty and the power to create, awaken, and destroy is indeed a complex and intriguing aspect of life. In philosophy, this concept can be explored through existentialism or even through the study of ethics and morality. In tech, this balance can be seen in the development of artificial intelligence or in the ethical considerations of new technologies. And in space, the unknown and the potential for discovery and exploration highlight this delicate balance as well. It's truly fascinating how these themes intersect across different disciplines.\\nHuman: Do you remember? What is my name?\\nAI: Yes, your name is Dipesh.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load history as a dictionary!\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14219b70",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The conversation is stored using this conversation buffer.\n",
    "# Let's see this\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a36e9905",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can add context like that!\n",
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61631b1f",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2fdf9ec",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ca79256",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "890a4497",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98e9ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Problems with classical memory!\n",
    "\n",
    "As we see, LLMs are stateless! Chat bots appear to have memory by providing **entire conversation** as **context** to the next API call to LLM.\n",
    "\n",
    "But this memory storage can be huge as the conversation grows, and prompting the LLM with this huge context (tokens) every time can cost us a lot!\n",
    "\n",
    "Hence, Langchain provides convenient Memory types to store the conversations efficiently, let's explore them!!\n",
    "\n",
    "## ConversationBufferWindowMemory\n",
    "\n",
    "`ConversationBufferWindowMemory` is a type of memory, that cuts the history of conversation and remembers only `k` recent conversation exchanges ***(Human-AI pairs)***, this way it ensures the conversation memory doesn't grow out of the limit!\n",
    "\n",
    "Let's see this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66eeccc3",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ea6233e",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# K = 1, keep only 1 recent exchange!\n",
    "memory = ConversationBufferWindowMemory(k=1)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc4553fb",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's manually store 2 converstation exchanges!\n",
    "# It will drop the earlier one...\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a788403",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4087bc87",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Conversation Chain again!\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True # See how it constantly drops earlier chats!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4faaa952",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, I'm Dipesh! I love space!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Dipesh! I'm glad to hear that you love space. There are so many fascinating things to learn about the universe. Did you know that the closest star to Earth is Proxima Centauri, which is part of the Alpha Centauri star system? It's located about 4.24 light-years away from us. Isn't that amazing?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It won't remember this too long 😥\n",
    "conversation.predict(input=\"Hi, I'm Dipesh! I love space!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb20ddaa",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, I'm Dipesh! I love space!\n",
      "AI: Hello Dipesh! I'm glad to hear that you love space. There are so many fascinating things to learn about the universe. Did you know that the closest star to Earth is Proxima Centauri, which is part of the Alpha Centauri star system? It's located about 4.24 light-years away from us. Isn't that amazing?\n",
      "Human: Yeah! That's really breath-taking!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm glad you think so! There are also many interesting exoplanets that have been discovered in recent years. For example, the exoplanet Kepler-186f is located about 500 light-years away from Earth and is considered to be in the habitable zone of its star. It's exciting to think about the possibility of life beyond our solar system, don't you think?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Yeah! That's really breath-taking!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "489b2194",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Yeah! That's really breath-taking!\n",
      "AI: I'm glad you think so! There are also many interesting exoplanets that have been discovered in recent years. For example, the exoplanet Kepler-186f is located about 500 light-years away from Earth and is considered to be in the habitable zone of its star. It's exciting to think about the possibility of life beyond our solar system, don't you think?\n",
      "Human: I believe so!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"There are also other fascinating exoplanets like TRAPPIST-1e, which is one of seven Earth-sized planets orbiting the ultra-cool dwarf star TRAPPIST-1. TRAPPIST-1e is located in the habitable zone of its star and could potentially have liquid water on its surface. The search for exoplanets and the study of their potential habitability is a rapidly growing field in astronomy. It's truly amazing to think about the diversity of worlds that exist beyond our own solar system.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I believe so!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4714485",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: I believe so!\n",
      "AI: There are also other fascinating exoplanets like TRAPPIST-1e, which is one of seven Earth-sized planets orbiting the ultra-cool dwarf star TRAPPIST-1. TRAPPIST-1e is located in the habitable zone of its star and could potentially have liquid water on its surface. The search for exoplanets and the study of their potential habitability is a rapidly growing field in astronomy. It's truly amazing to think about the diversity of worlds that exist beyond our own solar system.\n",
      "Human: Who am i?!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have the ability to determine your identity.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It doesn't have the context now!\n",
    "# Fascinating to know how our brains are so powerfull!!\n",
    "conversation.predict(input=\"Who am i?!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2931b92",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory\n",
    "\n",
    "A type of memory that limits the **number of tokens** stored in the conversation! \n",
    "\n",
    "This can directly relate with the **cost calculations** of an **LLM via tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f6d063c",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenizer for OpenAI models.\n",
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb9020ed",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43582ee6",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating TokenBufferMemory with 50 max_token_limit\n",
    "# This needs an LLM, to be able to calculate tokens accordingly!\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "\n",
    "# Saving the context\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "284288e1",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) # See history!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf4f0f",
   "metadata": {
    "height": 30
   },
   "source": [
    "We can see, it has **cut down** the first question!\n",
    "Let's increase the token limit!\n",
    "\n",
    "\n",
    "To our surprise, it doesn't keeps conversation cut-down in mid!\n",
    "It keeps the Conversation, correctly understandable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4ec8b7e",
   "metadata": {
    "height": 217
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating TokenBufferMemory with 100 max_token_limit\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=28)\n",
    "\n",
    "# Saving the context\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff55d5d",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory\n",
    "\n",
    "A type of memory that also keeps the conversation up to a given **max token limit** but **without cutting down** the conversation exchanges.\n",
    "\n",
    "Instead, what it does is, when the conversation **overflows** the max tokens, it automatically reduces the conversation to the given tokens by generating a **Summary** of the entire conversation using the LLM itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72dcf8b1",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a5b238f",
   "metadata": {
    "height": 268,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "# Add the conversation history\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e4ecabe",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human and AI exchange greetings and discuss the schedule for the day, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI provides details on each event and emphasizes the power of LangChain as a tool.'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) # It generated the summary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6728edba",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's test this using a Conversation Chain!\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a221b1d",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI exchange greetings and discuss the schedule for the day, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI provides details on each event and emphasizes the power of LangChain as a tool.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'For the meeting with the product team, a demo showcasing the latest features and updates on the LangChain project would be ideal. This could include a live demonstration of how LangChain streamlines language translation processes, improves accuracy, and increases efficiency. Additionally, highlighting any recent success stories or case studies would help illustrate the impact of LangChain in real-world scenarios.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting the conversation.\n",
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "023d741c",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI exchange greetings and discuss the schedule for the day, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI provides details on each event and emphasizes the power of LangChain as a tool.\n",
      "Human: What would be a good demo to show?\n",
      "AI: For the meeting with the product team, a demo showcasing the latest features and updates on the LangChain project would be ideal. This could include a live demonstration of how LangChain streamlines language translation processes, improves accuracy, and increases efficiency. Additionally, highlighting any recent success stories or case studies would help illustrate the impact of LangChain in real-world scenarios.\n",
      "Human: That's great, what should i focus more on?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'For the LangChain project work, focusing on the technical aspects of the platform would be beneficial. This could involve discussing the algorithms and machine learning models used in language translation, the data processing pipeline, and any recent improvements or optimizations made to enhance performance. Additionally, highlighting any upcoming features or enhancements in the pipeline could generate excitement and interest from the team.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again let's ask something!\n",
    "conversation.predict(input=\"That's great, what should i focus more on?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb582617",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human and AI discuss the schedule for the day, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI provides details on each event and emphasizes the power of LangChain as a tool. For the meeting with the product team, a demo showcasing the latest features and updates on the LangChain project would be ideal, including a live demonstration of how LangChain streamlines language translation processes, improves accuracy, and increases efficiency. Highlighting recent success stories or case studies would help illustrate the impact of LangChain in real-world scenarios.\\nHuman: That's great, what should i focus more on?\\nAI: For the LangChain project work, focusing on the technical aspects of the platform would be beneficial. This could involve discussing the algorithms and machine learning models used in language translation, the data processing pipeline, and any recent improvements or optimizations made to enhance performance. Additionally, highlighting any upcoming features or enhancements in the pipeline could generate excitement and interest from the team.\"}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c327991",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"That's great, what should i focus more on?\", additional_kwargs={}, example=False),\n",
       " AIMessage(content='For the LangChain project work, focusing on the technical aspects of the platform would be beneficial. This could involve discussing the algorithms and machine learning models used in language translation, the data processing pipeline, and any recent improvements or optimizations made to enhance performance. Additionally, highlighting any upcoming features or enhancements in the pipeline could generate excitement and interest from the team.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's show the buffer!\n",
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6709b89-182e-4b7b-b811-1584dda20652",
   "metadata": {},
   "source": [
    "It's been great!! I read the Langchain documentation also! You should too!\n",
    "\n",
    "Reminder: Download your notebook to you local computer to save your work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
